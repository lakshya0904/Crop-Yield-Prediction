{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Engineer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPvDp3MM3JhTh52s6SojefM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"zYULPLuEFD0P"},"source":["import pandas as pd\r\n","import numpy as np\r\n","import os\r\n","from os import listdir\r\n","from os.path import isfile, join\r\n","import gdal"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"213bbxrdFGYP"},"source":["    def load_clean_yield_data(yield_data_filepath):\r\n","        \"\"\"\r\n","        Cleans the yield data by making sure any Nan values in the columns we care about\r\n","        are removed\r\n","        \"\"\"\r\n","        important_columns=['Year', 'State ANSI', 'County ANSI', 'Value']\r\n","        yield_data=pd.read_csv(yield_data_filepath).dropna(subset=important_columns,how='any')\r\n","\r\n","        return yield_data\r\n","\r\n","    def get_tif_files(image_path):\r\n","        \"\"\"\r\n","        Get all the .tif files in the image folder.\r\n","\r\n","        Parameters\r\n","        ----------\r\n","        image_path: pathlib Path\r\n","            Directory to search for tif files\r\n","        Returns:\r\n","            A list of .tif filenames\r\n","        \"\"\"\r\n","        files = []\r\n","        for dir_file in image_path.iterdir():\r\n","            if str(dir_file).endswith('tif'):\r\n","                files.append(str(dir_file.parts[-1]))\r\n","        return files\r\n","    def get_tif_files_12(image_path):#name length less than 12\r\n","        files = []\r\n","        for dir_file in image_path.iterdir():\r\n","            if str(dir_file).endswith('tif'):\r\n","                if len(str(dir_file.parts[-1]))<12:#max possible name is ss_ccc.tif, len=10\r\n","                    files.append(str(dir_file.parts[-1]))\r\n","        return files\r\n","    def get_tif_files_12(mask_path,temperature_path,image_path,weather_path):\r\n","        mask_files=[f for f in listdir(mask_path) if isfile(join(mask_path, f))]\r\n","        temperature_files=[f for f in listdir(temperature_path) if isfile(join(temperature_path, f))]\r\n","        image_files=[f for f in listdir(image_path) if isfile(join(image_path, f))]\r\n","        weather_files=[f for f in listdir(weather_path) if isfile(join(weather_path, f))]\r\n","        files = []\r\n","        for dir_file in image_path.iterdir():\r\n","            if str(dir_file).endswith('tif'):\r\n","                if len(str(dir_file.parts[-1]))<12:#max possible name is ss_ccc.tif, len=10\r\n","                    if str(dir_file.parts[-1]) in temperature_files:\r\n","                        if str(dir_file.parts[-1]) in weather_files:\r\n","                            if str(dir_file.parts[-1]) in mask_files:\r\n","                                files.append(str(dir_file.parts[-1]))\r\n","        return files\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Azdae83F8OkJ"},"source":["#feature engineering goes here\r\n","from pathlib import Path\r\n","import numpy as np\r\n","import pandas as pd\r\n","import math\r\n","\r\n","class Engineer:\r\n","    \"\"\"\r\n","    Take the preprocessed data from the Data Cleaner\r\n","    and turn the images into matrices which can be input\r\n","    into the machine learning models.\r\n","\r\n","    These matrices can either be histograms, which describe the distributions of\r\n","    pixels on each band, and contain 32 bins.\r\n","    This turns the band of an image from dim=(width*height) to dim=32.\r\n","\r\n","    They can also be means of each band, which turns the band of an image from\r\n","    dim=(width*height) into a scalar value.\r\n","    \"\"\"\r\n","    def __init__(self, cleaned_data_path=Path('img_output'),\r\n","                 yield_data_filepath=Path('SOP- Time Series Analysis with Deep Learning/yield_data_less_entries.csv'),\r\n","                 county_data_filepath=Path('SOP- Time Series Analysis with Deep Learning/county_data.csv')):\r\n","        self.cleaned_data=cleaned_data_path\r\n","        self.processed_files=global_processed_files\r\n","        print(len(self.processed_files))\r\n","        #merge the yield and county data for easier manipulation\r\n","        yield_data=load_clean_yield_data(yield_data_filepath)[['Year','State ANSI','County ANSI','Value']]\r\n","        yield_data.columns=['Year','State','County','Value']\r\n","        county_data=pd.read_csv(county_data_filepath)[['CntyFips','StateFips', 'Longitude','Latitude']]\r\n","        county_data.columns=['County', 'State', 'Longitude', 'Latitude']\r\n","        self.yield_data=yield_data.merge(county_data, how='left', on=['County','State'])\r\n","\r\n","    def get_filenames(self):\r\n","        \"\"\"\r\n","        Get all the .tif files in the image folder.\r\n","        \"\"\"\r\n","        files=[]\r\n","        \r\n","        print(self.cleaned_data)\r\n","        for dir_file in Path(self.cleaned_data).iterdir():\r\n","            if str(dir_file).endswith('npy'):\r\n","                #strip out the directory so it's just file name\r\n","                files.append(str(dir_file.parts[-1]))\r\n","                global_processed_files.append(str(dir_file.parts[-1]))\r\n","        # files.sort()#to ensure that soil and image data stored in same sequence\r\n","        # global_processed_files=files\r\n","        return files\r\n","    \r\n","    @staticmethod\r\n","    def filter_timespan(imcol, start_day=91, end_day=335, composite_period=8, bands=11):\r\n","        # 1st April to 30th Nov\r\n","        \"\"\"\r\n","        Given an image collection containing a year's worth of data,\r\n","        filter it between start_day and end_day. If end_day is later than the date\r\n","        for which we have data, the image collection is padded with zeros.\r\n","\r\n","        Parameters\r\n","        ----------\r\n","        imcol: The image collection to be filtered\r\n","        start_day: int, default=49\r\n","            The earliest day for which to consider data\r\n","        end_day: int, default=305\r\n","            The last day for which to consider data\r\n","        composite_period: int, default=8\r\n","            The composite period of the images. Default taken from the composite\r\n","            periods of the MOD09A1 and MYD11A2 datasets\r\n","        bands: int, default=9\r\n","            The number of bands per image. Default taken from the number of bands in the\r\n","            MOD09A1 + the number of bands in the MYD11A2 datasets\r\n","\r\n","        Returns\r\n","        ----------\r\n","        A filtered image collection\r\n","        \"\"\"\r\n","        start_index=int(math.floor(start_day/composite_period))*bands\r\n","        end_index=int(math.floor(end_day/composite_period))*bands\r\n","\r\n","        if end_index>imcol.shape[2]:\r\n","            padding=np.zeros((imcol.shape[0], imcol.shape[1], end_index-imcol.shape[2]))\r\n","            imcol=np.concatenate((imcol,padding), axis=2)\r\n","        return imcol[:,:, start_index:end_index]\r\n","\r\n","    @staticmethod\r\n","    def _calculate_histogram(imagecol, num_bins=32, bands=11, max_bin_val=4999,\r\n","                             channels_first=True):\r\n","        \"\"\"\r\n","        Given an image collection, turn it into a histogram.\r\n","\r\n","        Parameters\r\n","        ----------\r\n","        imcol: The image collection to be histogrammed\r\n","        num_bins: int, default=32\r\n","            The number of bins to use in the histogram.\r\n","        bands: int, default=9\r\n","            The number of bands per image. Default taken from the number of bands in the\r\n","            MOD09A1 + the number of bands in the MYD11A2 datasets\r\n","        max_bin_val: int, default=4999\r\n","            The maximum value of the bins. The default is taken from the original repository;\r\n","            note that the maximum pixel values from the MODIS datsets range from 16000 to\r\n","            18000 depending on the band\r\n","\r\n","        Returns\r\n","        ----------\r\n","        A histogram for each band, of the band's pixel values. The output shape is\r\n","        [num_bins, times, bands], where times is the number of unique timestamps in the\r\n","        image collection.\r\n","        \"\"\"\r\n","        bin_seq=np.linspace(1,max_bin_val, num_bins+1)\r\n","\r\n","        hist=[]\r\n","        for im in np.split(imagecol, imagecol.shape[-1]/bands, axis=-1):\r\n","            imhist=[]\r\n","            for i in range(im.shape[-1]):\r\n","                density,_=np.histogram(im[:,:,i],bin_seq, density=False)\r\n","                # max prevents divide by zero\r\n","                imhist.append(density/max(1,density.sum()))\r\n","            if channels_first:\r\n","                hist.append(np.stack(imhist))\r\n","            else:\r\n","                hist.append(np.stack(imhist,axis=1))\r\n","        return np.stack(hist, axis=1)\r\n","\r\n","    def process(self, num_bands=11, generate='histogram', num_bins=32, max_bin_val=4999,\r\n","                channels_first=True):\r\n","        \"\"\"\r\n","        Parameters\r\n","        ----------\r\n","        num_bands: int, default=9\r\n","            The number of bands per image. Default taken from the number of bands in the\r\n","            MOD09A1 + the number of bands in the MYD11A2 datasets\r\n","        generate: str, {'mean', 'histogram'}, default='mean'\r\n","            What to generate from the data. If 'mean', calculates a mean\r\n","            of all the bands. If 'histogram', calculates a histogram of all\r\n","            the bands with num_bins bins for each band.\r\n","        num_bins: int, default=32\r\n","            If generate=='histogram', the number of bins to generate in the histogram.\r\n","        max_bin_val: int, default=4999\r\n","            The maximum value of the bins. The default is taken from the original repository;\r\n","            note that the maximum pixel values from the MODIS datsets range from 16000 to\r\n","            18000 depending on the band\r\n","        channels_first: boolean, default=True\r\n","            If true, the output histogram has shape [bands, times, bins]. Otherwise, it\r\n","            has shape [times, bins, bands]\r\n","        \"\"\"\r\n","        #define all the outputs of this method\r\n","        output_images=[]\r\n","        yields=[]\r\n","        years=[]\r\n","        locations=[]\r\n","        state_county_info=[]\r\n","        count=0\r\n","        for yield_data in self.yield_data.itertuples():\r\n","            year=yield_data.Year\r\n","            county=yield_data.County\r\n","            state=yield_data.State\r\n","\r\n","            filename=f'{year}_{int(state)}_{int(county)}.npy'\r\n","            if filename in self.processed_files:\r\n","                image=np.load(self.cleaned_data/filename)\r\n","                image=self.filter_timespan(image, start_day=91, end_day=335,\r\n","                                           bands=num_bands)\r\n","                # from 1st April to 30th Nov\r\n","                print(filename,\"\\t\",image.shape)\r\n","                if generate=='mean':\r\n","                    image=np.sum(image, axis=(0,1))/np.count_nonzero(image)*image.shape[2]\r\n","                    image[np.isnan(image)]=0\r\n","                elif generate=='histogram':\r\n","                    image=self._calculate_histogram(image, bands=num_bands,\r\n","                                                    num_bins=num_bins,\r\n","                                                    max_bin_val=max_bin_val,\r\n","                                                    channels_first=channels_first)\r\n","                    \r\n","                output_images.append(image)\r\n","                yields.append(yield_data.Value)\r\n","                years.append(year)\r\n","\r\n","                # some of the minus signs in the longitudes have been carried over to the\r\n","                # longitudes, i.e. 19.683885, -155.393159 becomes 19.683885-, 155.393159\r\n","                try:\r\n","                    lat, lon=float(yield_data.Latitude), float(yield_data.Longitude)\r\n","                except ValueError:\r\n","                    lat=float(yield_data.Latitude[:-1])\r\n","                    lon=-float(yield_data.Longitude)\r\n","                locations.append(np.array([lon,lat]))\r\n","                state_county_info.append(np.array([int(state), int(county)]))\r\n","                \r\n","        np.savez(self.cleaned_data/f'histogram_all_{\"mean\" if(generate==\"mean\") else \"full\"}.npz',\r\n","                 output_image=np.stack(output_images), output_yield=np.array(yields),\r\n","                 output_year=np.array(years), output_locations=np.stack(locations),\r\n","                 output_index=np.stack(state_county_info))\r\n","        print(f'Finished generating image {generate}s!')\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IU1LzY768oVY"},"source":["\r\n","cleaned_data_path=Path('/content/drive/MyDrive/cleanedData/imageComposite')\r\n","yield_data_path = Path('/content/drive/MyDrive/data_Lakshya/corn_yield_Lakshya.csv')\r\n","county_data_path = Path('/content/drive/MyDrive/data_Lakshya/county_data.csv')\r\n","num_bins=32\r\n","max_bin_val=5000\r\n","\r\n","engineer = Engineer(cleaned_data_path, yield_data_path, county_data_path)\r\n","engineer.process(num_bands=11, generate='histogram', num_bins=num_bins, max_bin_val=max_bin_val,\r\n","                channels_first=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OykhmXRbQ29p","executionInfo":{"status":"ok","timestamp":1610684372892,"user_tz":-330,"elapsed":1458,"user":{"displayName":"LAKSHYA AGARWAL","photoUrl":"","userId":"16562277029609822402"}},"outputId":"12c5e8a7-131f-4828-e3c6-e994cf89bd2f"},"source":["import numpy as np\r\n","bin_seq=np.linspace(1,5000, 32+1)\r\n","print(bin_seq)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[1.00000000e+00 1.57218750e+02 3.13437500e+02 4.69656250e+02\n"," 6.25875000e+02 7.82093750e+02 9.38312500e+02 1.09453125e+03\n"," 1.25075000e+03 1.40696875e+03 1.56318750e+03 1.71940625e+03\n"," 1.87562500e+03 2.03184375e+03 2.18806250e+03 2.34428125e+03\n"," 2.50050000e+03 2.65671875e+03 2.81293750e+03 2.96915625e+03\n"," 3.12537500e+03 3.28159375e+03 3.43781250e+03 3.59403125e+03\n"," 3.75025000e+03 3.90646875e+03 4.06268750e+03 4.21890625e+03\n"," 4.37512500e+03 4.53134375e+03 4.68756250e+03 4.84378125e+03\n"," 5.00000000e+03]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gDPt0HQV8oR4"},"source":["import numpy as np\r\n","data = np.load('/content/drive/MyDrive/cleanedData/histImageComposite/histogram_all_full.npz')\r\n","lst = data.files\r\n","np.set_printoptions(threshold=np. inf)\r\n","\r\n","#Required output in the form (1(years), 32(bins), 11(layers), days) and currently shape is (years, layers, days, bins(32))\r\n","#required (10,32,11,19)\r\n","\r\n","for item in lst:\r\n","    print(item)\r\n","    a = data[item][0]\r\n","    print(data[item][0].shape)\r\n","    print(data[item][0])\r\n","print(len(data[\"output_index\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Am4IHFQy8oPe"},"source":["out_img_composite = data['output_image']\r\n","out_img_composite.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ZNi4rrl8oNL"},"source":["#data preparation IMPORTANT TRANSPOSE\r\n","out_img_composite = np.transpose(out_img_composite, axes=(0,2,3,1))\r\n","out_img_composite.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYvUsHf58oKZ"},"source":["out_img_composite = out_img_composite.reshape(out_img_composite.shape[0],out_img_composite.shape[1],1,out_img_composite.shape[2],out_img_composite.shape[3])\r\n","out_img_composite.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8MbIUOzp84GG"},"source":["### Engineering for Soil"]},{"cell_type":"code","metadata":{"id":"izUngGXa8oHn"},"source":["from pathlib import Path\r\n","import numpy as np\r\n","import pandas as pd\r\n","import math\r\n","\r\n","class EngineerSoil:\r\n","    \"\"\"\r\n","    Take the preprocessed data from the Data Cleaner\r\n","    and turn the images into matrices which can be input\r\n","    into the machine learning models.\r\n","\r\n","    These matrices can either be histograms, which describe the distributions of\r\n","    pixels on each band, and contain 32 bins.\r\n","    This turns the band of an image from dim=(width*height) to dim=32.\r\n","\r\n","    They can also be means of each band, which turns the band of an image from\r\n","    dim=(width*height) into a scalar value.\r\n","    \"\"\"\r\n","    def __init__(self, cleaned_data_path=Path('img_output'),\r\n","                 yield_data_filepath=Path('SOP- Time Series Analysis with Deep Learning/yield_data_less_entries.csv'),\r\n","                 county_data_filepath=Path('SOP- Time Series Analysis with Deep Learning/county_data.csv')):\r\n","        self.cleaned_data=cleaned_data_path\r\n","        self.processed_files=global_processed_files\r\n","        print(len(self.processed_files))\r\n","        print(self.processed_files)\r\n","        #merge the yield and county data for easier manipulation\r\n","        yield_data=load_clean_yield_data(yield_data_filepath)[['Year','State ANSI','County ANSI','Value']]\r\n","        yield_data.columns=['Year','State','County','Value']\r\n","        county_data=pd.read_csv(county_data_filepath)[['CntyFips','StateFips', 'Longitude','Latitude']]\r\n","        county_data.columns=['County', 'State', 'Longitude', 'Latitude']\r\n","        self.yield_data=yield_data.merge(county_data, how='left', on=['County','State'])\r\n","\r\n","    def get_filenames(self):\r\n","        \"\"\"\r\n","        Get all the .tif files in the image folder.\r\n","        \"\"\"\r\n","        files=[]\r\n","        files.sort()#to ensure that soil and image data stored in same sequence\r\n","        for dir_file in Path(self.cleaned_data).iterdir():\r\n","            if str(dir_file).endswith('npy'):\r\n","                if str(dir_file.parts[-1]) in global_processed_files:\r\n","                    files.append(str(dir_file.parts[-1]))\r\n","        return files\r\n","    \r\n","    @staticmethod\r\n","    def filter_timespan(imcol, start_day=49, end_day=305, composite_period=8, bands=9):\r\n","        \"\"\"\r\n","        Given an image collection containing a year's worth of data,\r\n","        filter it between start_day and end_day. If end_day is later than the date\r\n","        for which we have data, the image collection is padded with zeros.\r\n","\r\n","        Parameters\r\n","        ----------\r\n","        imcol: The image collection to be filtered\r\n","        start_day: int, default=49\r\n","            The earliest day for which to consider data\r\n","        end_day: int, default=305\r\n","            The last day for which to consider data\r\n","        composite_period: int, default=8\r\n","            The composite period of the images. Default taken from the composite\r\n","            periods of the MOD09A1 and MYD11A2 datasets\r\n","        bands: int, default=9\r\n","            The number of bands per image. Default taken from the number of bands in the\r\n","            MOD09A1 + the number of bands in the MYD11A2 datasets\r\n","\r\n","        Returns\r\n","        ----------\r\n","        A filtered image collection\r\n","        \"\"\"\r\n","        start_index=int(math.floor(start_day/composite_period))*bands\r\n","        end_index=int(math.floor(end_day/composite_period))*bands\r\n","\r\n","        if end_index>imcol.shape[2]:\r\n","            padding=np.zeros((imcol.shape[0], imcol.shape[1], end_index-imcol.shape[2]))\r\n","            imcol=np.concatenate((imcol,padding), axis=2)\r\n","        return imcol[:,:, start_index:end_index]\r\n","\r\n","    @staticmethod\r\n","    def _calculate_histogram(imagecol, num_bins=32, bands=11, max_bin_val=4999,\r\n","                             channels_first=True):\r\n","        \"\"\"\r\n","        Given an image collection, turn it into a histogram.\r\n","\r\n","        Parameters\r\n","        ----------\r\n","        imcol: The image collection to be histogrammed\r\n","        num_bins: int, default=32\r\n","            The number of bins to use in the histogram.\r\n","        bands: int, default=9\r\n","            The number of bands per image. Default taken from the number of bands in the\r\n","            MOD09A1 + the number of bands in the MYD11A2 datasets\r\n","        max_bin_val: int, default=4999\r\n","            The maximum value of the bins. The default is taken from the original repository;\r\n","            note that the maximum pixel values from the MODIS datsets range from 16000 to\r\n","            18000 depending on the band\r\n","\r\n","        Returns\r\n","        ----------\r\n","        A histogram for each band, of the band's pixel values. The output shape is\r\n","        [num_bins, times, bands], where times is the number of unique timestamps in the\r\n","        image collection.\r\n","        \"\"\"\r\n","        bin_seq=np.linspace(1,max_bin_val, num_bins+1)\r\n","\r\n","        hist=[]\r\n","        for im in np.split(imagecol, imagecol.shape[-1]/bands, axis=-1):\r\n","            imhist=[]\r\n","            for i in range(im.shape[-1]):\r\n","                density,_=np.histogram(im[:,:,i],bin_seq, density=False)\r\n","                # max prevents divide by zero\r\n","                imhist.append(density/max(1,density.sum()))\r\n","            if channels_first:\r\n","                hist.append(np.stack(imhist))\r\n","            else:\r\n","                hist.append(np.stack(imhist,axis=1))\r\n","        return np.stack(hist, axis=1)\r\n","\r\n","    def process(self, num_bands=36, generate='histogram', num_bins=32, max_bin_val=4999,\r\n","                channels_first=True):\r\n","        \"\"\"\r\n","        Parameters\r\n","        ----------\r\n","        num_bands: int, default=9\r\n","            The number of bands per image. Default taken from the number of bands in the\r\n","            MOD09A1 + the number of bands in the MYD11A2 datasets\r\n","        generate: str, {'mean', 'histogram'}, default='mean'\r\n","            What to generate from the data. If 'mean', calculates a mean\r\n","            of all the bands. If 'histogram', calculates a histogram of all\r\n","            the bands with num_bins bins for each band.\r\n","        num_bins: int, default=32\r\n","            If generate=='histogram', the number of bins to generate in the histogram.\r\n","        max_bin_val: int, default=4999\r\n","            The maximum value of the bins. The default is taken from the original repository;\r\n","            note that the maximum pixel values from the MODIS datsets range from 16000 to\r\n","            18000 depending on the band\r\n","        channels_first: boolean, default=True\r\n","            If true, the output histogram has shape [bands, times, bins]. Otherwise, it\r\n","            has shape [times, bins, bands]\r\n","        \"\"\"\r\n","\r\n","        #define all the outputs of this method\r\n","        output_images=[]\r\n","        yields=[]\r\n","        years=[]\r\n","        locations=[]\r\n","        state_county_info=[]\r\n","        count=0\r\n","        for yield_data in self.yield_data.itertuples():\r\n","            year=yield_data.Year\r\n","            county=yield_data.County\r\n","            state=yield_data.State\r\n","\r\n","            filename=f'{year}_{int(state)}_{int(county)}.npy'\r\n","            if filename in self.processed_files:\r\n","                image=np.load(self.cleaned_data/filename)\r\n","                print(filename,\"\\t\",image.shape)\r\n","                if generate=='mean':\r\n","                    image=np.sum(image, axis=(0,1))/np.count_nonzero(image)*image.shape[2]\r\n","                    image[np.isnan(image)]=0\r\n","                elif generate=='histogram':\r\n","                    image=self._calculate_histogram(image, bands=num_bands,\r\n","                                                    num_bins=num_bins,\r\n","                                                    max_bin_val=max_bin_val,\r\n","                                                    channels_first=channels_first)\r\n","                    \r\n","                output_images.append(image)\r\n","                yields.append(yield_data.Value)\r\n","                years.append(year)\r\n","\r\n","                # some of the minus signs in the longitudes have been carried over to the\r\n","                # longitudes, i.e. 19.683885, -155.393159 becomes 19.683885-, 155.393159\r\n","                try:\r\n","                    lat, lon=float(yield_data.Latitude), float(yield_data.Longitude)\r\n","                except ValueError:\r\n","                    lat=float(yield_data.Latitude[:-1])\r\n","                    lon=-float(yield_data.Longitude)\r\n","                locations.append(np.array([lon,lat]))\r\n","\r\n","                state_county_info.append(np.array([int(state), int(county)]))\r\n","\r\n","        np.savez(self.cleaned_data/f'histogram_all_{\"mean\" if(generate==\"mean\") else \"full\"}.npz',\r\n","                 output_image=np.stack(output_images), output_yield=np.array(yields),\r\n","                 output_year=np.array(years), output_locations=np.stack(locations),\r\n","                 output_index=np.stack(state_county_info))\r\n","        print(f'Finished generating image {generate}s!')\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQOEicCG8oE8"},"source":["# cleaned_data_path = Path('/content/drive/MyDrive/singleData')\r\n","cleaned_data_path=Path('/content/drive/MyDrive/cleanedData/histSoilComposite')\r\n","yield_data_path = Path('/content/drive/MyDrive/data_Lakshya/corn_yield_Lakshya.csv')\r\n","county_data_path = Path('/content/drive/MyDrive/data_Lakshya/county_data.csv')\r\n","num_bins=32\r\n","max_bin_val=4999\r\n","\r\n","engineerSoil = EngineerSoil(cleaned_data_path, yield_data_path, county_data_path)\r\n","engineerSoil.process(num_bands=36, generate='histogram', num_bins=num_bins, max_bin_val=max_bin_val,\r\n","                channels_first=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOkzPYzb8n_7"},"source":["data = np.load('/content/drive/MyDrive/cleanedData/soilComposite/histogram_all_full.npz')\r\n","lst = data.files\r\n","np.set_printoptions(threshold=np. inf)\r\n","\r\n","#Required output in the form (1(years), 32(bins), 11(layers), days) and currently shape is (years, layers, days, bins(32))\r\n","#required (10,32,11,19)\r\n","\r\n","for item in lst:\r\n","    print(item)\r\n","    a = data[item][0]\r\n","    print(data[item][0].shape)\r\n","print(len(data[\"output_index\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDKFPNz39Fqs"},"source":["out_soil = data['output_image']\r\n","out_soil.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l6CbfYrN9FnV"},"source":["out_soil = np.transpose(out_soil, axes=(0,2,3,1))\r\n","out_soil.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yukdHOCt9Fkg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1qbstqWX9Fh3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vIZFD_FV9FfY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KQS-Cbpr9FcI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_QD81Q849FZY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7r_IfMah9FWk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFiGbfdL9FUA"},"source":[""],"execution_count":null,"outputs":[]}]}